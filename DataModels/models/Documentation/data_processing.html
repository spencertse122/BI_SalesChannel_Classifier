<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>data_processing API documentation</title>
<meta name="description" content="This is a module that houses all the pandas data manipulation process, including cleaning, encoding,
shape changing, and more. The purpose of â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>data_processing</code></h1>
</header>
<section id="section-intro">
<p>This is a module that houses all the pandas data manipulation process, including cleaning, encoding,
shape changing, and more. The purpose of separating the following functionalities, is to ensure the code
is reusable, and to support scalable future projects. Each sub-class is a particular view for a specific
usage. (e.g.
feed_data_decision_tree is a machine learning model ready setup)</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
This is a module that houses all the pandas data manipulation process, including cleaning, encoding, 
shape changing, and more. The purpose of separating the following functionalities, is to ensure the code 
is reusable, and to support scalable future projects. Each sub-class is a particular view for a specific 
usage. (e.g.  feed_data_decision_tree is a machine learning model ready setup)
&#34;&#34;&#34;

# Importing libraries
import pandas as pd
from pandas.api.types import is_string_dtype
from pandas.api.types import is_numeric_dtype
import numpy as np

import os
import glob
from datetime import datetime 



cleaningDict = {
    &#39;64150, MO&#39; : &#39;RIVERSIDE, MO&#39;,
    &#39;95678, CA&#39; : &#39;ROSEVILLE, CA&#39;,
    &#39;15801, PA&#39; : &#39;DUBOIS, PA&#39;
}

&#34;&#34;&#34;
Initiating all the reference data first for usages. Creating a reference data for cleaning zip code manually for now
since we don&#39;t have an approved API for this project.
&#34;&#34;&#34;

binary_encoding_map = {
    &#39;Y&#39; : 1,
    &#39;N&#39; : 0,
    &#39;Engaged&#39; : 1,
    &#39;Not engaged&#39; : 0
}

&#34;&#34;&#34;
Intiating the mapping for conversion
&#34;&#34;&#34;

driver_columns = [
    &#39;email_open&#39;, &#39;web_visit&#39;, 
    &#39;webcast_attendee&#39;, &#39;marketing_engaged&#39;
    ]

&#34;&#34;&#34;
Creating a binary column list for ease of usages
&#34;&#34;&#34;

#Creating all the data loading methods
# Defining data loading functions

def read_file(data_file):
    &#34;&#34;&#34;
    Function to read one data file with the custom setup
    &#34;&#34;&#34;
    df = pd.read_csv(data_file, delimiter=&#34;;&#34;)
    return df

def read_all_files(data_folder):
    &#34;&#34;&#34;
    Function to get all the data files of a target type, and
    &#34;&#34;&#34;
    # Scanning for all the data files
    dataPaths = glob.glob(data_folder + &#34;/*.txt&#34;)

    # Loading in the data
    listOfFrames = []
    for i in dataPaths:
        tdf = pd.read_csv(i, sep=&#34;;&#34;)
        tdf[&#39;source_file&#39;] = i  # Adding source file as a column for ease of tracking
        listOfFrames.append(tdf)
        
    # Combining all the dataframes
    df = pd.concat(listOfFrames, ignore_index=True)
    return df


# Using a dictionary to map input methods
input_methods = {
    &#39;txt&#39; : read_file, # if it&#39;s a txt file, just use read csv
    &#39;&#39; : read_all_files # if it&#39;s a directory, use glob to read everything
}


class house_of_data(object):
    &#34;&#34;&#34;
    Collection of data with each method as a stage of data manipulation process or metrics output.
    &#34;&#34;&#34;

    def __init__(self, DataInput):
        # first validating the input to see which method to use
        inputType = os.path.splitext(DataInput)[-1]
        if inputType == &#34;.txt&#34;:
            read_method = input_methods.get(&#39;txt&#39;)
        else:
            read_method = input_methods.get(&#39;&#39;)

        # Actually initiating the object attributes
        self._DataInput = DataInput
        self._RawData = read_method(DataInput)

    def cleaning(self):
        &#34;&#34;&#34;
        Performing all the cleanings related to this dataset
        &#34;&#34;&#34;
        df = self._RawData.drop_duplicates().copy() #Creating a copy of the drop_dup dataframe
        # Run a for-loop to go through string columns
        # and strip the leading and trailing whitespaces

        for i in df.columns:
            if is_string_dtype(df[i]):   # if it&#39;s a string column
                df[i] = df[i].str.strip()   # strip out the white spaces
            else:
                pass
    
        # resetting index after the drop duplicates
        df = df.reset_index(drop=True)

        # Cleaning up the duplicated states in cell
        df[&#39;city_state&#39;] = df[&#39;city_state&#39;].str.replace(&#34;WASHINGTON, DC, DC&#34;, &#34;WASHINGTON DC, DC&#34;)
        df[&#39;city_state&#39;] = df[&#39;city_state&#39;].str.replace(&#34;KNOXVILLE, TN, TN&#34;, &#34;KNOXVILLE, TN&#34;)

        # Performing the zip code cleaning
        df[&#39;city_state&#39;] = df[&#39;city_state&#39;].replace(cleaningDict)

        # Saving it to the object
        self._clean_df = df

    def other_data_validations(self):
        &#34;&#34;&#34;
        Ensure new incoming data does not have new violations or needed updates.
        &#34;&#34;&#34;
        checker = []
        # Checking if the broker ID is number
        checker.append(is_numeric_dtype(self._clean_df[&#39;broker_name&#39;].str.split(&#34;Broker&#34;, expand=True)[1].astype(float)))
        # Checking if the broker name is correct
        checker.append(len(self._clean_df[&#39;broker_name&#39;].str.split(&#34;Broker&#34;, expand=True).columns) == 2)
        # Checking if the city states only has one comma
        checker.append(len(self._clean_df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True).columns) == 2)
        # Checking if the city state column contains numbers (zip codes)
        mask = self._clean_df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True)[0].str.isnumeric()
        checker.append(len(self._clean_df[mask]) == 0)
        # Checking if thet prefix is a character per describe
        mask1 = self._clean_df[&#39;territory&#39;].str[0].str.isalpha() == False
        mask2 = self._clean_df[&#39;territory&#39;].str[0] != &#34;I&#34;
        mask3 = self._clean_df[&#39;territory&#39;].str[0] != &#34;W&#34;
        checker.append(len(self._clean_df.loc[mask1, &#39;territory&#39;]) == 0)
        checker.append(len(self._clean_df.loc[mask2&amp;mask3, &#39;territory&#39;]) == 0)
        # Checking fund category column
        mask4 = self._clean_df[&#39;fund_category&#39;].str.isnumeric() == True
        checker.append(len(self._clean_df.loc[mask, &#39;fund_category&#39;]) == 0)
        checker.append(is_numeric_dtype(self._clean_df[&#39;firm_x_sales&#39;]))
        checker.append(is_numeric_dtype(self._clean_df[&#39;total_industry_sales&#39;]))

        # Checking binary columns
        bin_check = True
        for i in driver_columns:
            if len(self._clean_df[i].unique()) == 2:
                pass
            else:
                bin_check = False
                print(f&#34;{i} is not binary&#34;)
                break

        checker.append(bin_check)

        return checker


    def enrichment(self):
        &#34;&#34;&#34;
        Built on top of the clean_df, this is going to split out the analytics
        columns
        &#34;&#34;&#34;
        # Creating a copy of the clean dataframe for enrichment
        self.cleaning()
        df = self._clean_df.copy()

        # Binary encode all the binary categorical variables
        df[driver_columns] = df[driver_columns].replace(binary_encoding_map)
        
        # Splitting out the sates since the city_state information is too granular for overview
        df[&#39;state&#39;] = df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True)[1]
        
        # Splitting out the Channel
        df[&#39;i_or_w&#39;] =df[&#39;territory&#39;].str[0]
        I_OR_W = {
            &#34;I&#34; : 1,
            &#34;W&#34; : 0
        }

        df[&#39;i_or_w&#39;] = df[&#39;i_or_w&#39;].replace(I_OR_W)
        
        # Initiating a filter to exclude all the no sales generated record
        mask = df[&#39;firm_x_sales&#39;] &gt; 0
        significant_cut = df[mask][&#39;firm_x_sales&#39;].quantile(0.2)
        # Intiating a filter with the twenty percentile cut
        mask1 = df[&#39;firm_x_sales&#39;] &gt; significant_cut
        # Performing the filter and label
        df[&#39;effective_sale&#39;] = 0
        df.loc[mask1,&#39;effective_sale&#39;] = 1
        
        # Since driver_columns are the same as driver columns
        # we will utilize that to create driver_pattern
        df[&#39;driver_pattern&#39;] = df[driver_columns].apply(tuple,axis=1)
        
        df = df.reset_index(drop=True)
        # Save the enriched dataframe to the house
        self._enriched_df = df
    



    def get_metrics(self):
        &#34;&#34;&#34;
        Printing an output of quick metrics
        &#34;&#34;&#34;
        pass




if __name__ == &#34;__main__&#34;:
    import sys
    script, data_input = sys.args</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="data_processing.binary_encoding_map"><code class="name">var <span class="ident">binary_encoding_map</span></code></dt>
<dd>
<div class="desc"><p>Intiating the mapping for conversion</p></div>
</dd>
<dt id="data_processing.cleaningDict"><code class="name">var <span class="ident">cleaningDict</span></code></dt>
<dd>
<div class="desc"><p>Initiating all the reference data first for usages. Creating a reference data for cleaning zip code manually for now
since we don't have an approved API for this project.</p></div>
</dd>
<dt id="data_processing.driver_columns"><code class="name">var <span class="ident">driver_columns</span></code></dt>
<dd>
<div class="desc"><p>Creating a binary column list for ease of usages</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="data_processing.read_all_files"><code class="name flex">
<span>def <span class="ident">read_all_files</span></span>(<span>data_folder)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to get all the data files of a target type, and</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_all_files(data_folder):
    &#34;&#34;&#34;
    Function to get all the data files of a target type, and
    &#34;&#34;&#34;
    # Scanning for all the data files
    dataPaths = glob.glob(data_folder + &#34;/*.txt&#34;)

    # Loading in the data
    listOfFrames = []
    for i in dataPaths:
        tdf = pd.read_csv(i, sep=&#34;;&#34;)
        tdf[&#39;source_file&#39;] = i  # Adding source file as a column for ease of tracking
        listOfFrames.append(tdf)
        
    # Combining all the dataframes
    df = pd.concat(listOfFrames, ignore_index=True)
    return df</code></pre>
</details>
</dd>
<dt id="data_processing.read_file"><code class="name flex">
<span>def <span class="ident">read_file</span></span>(<span>data_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Function to read one data file with the custom setup</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_file(data_file):
    &#34;&#34;&#34;
    Function to read one data file with the custom setup
    &#34;&#34;&#34;
    df = pd.read_csv(data_file, delimiter=&#34;;&#34;)
    return df</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="data_processing.house_of_data"><code class="flex name class">
<span>class <span class="ident">house_of_data</span></span>
<span>(</span><span>DataInput)</span>
</code></dt>
<dd>
<div class="desc"><p>Collection of data with each method as a stage of data manipulation process or metrics output.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class house_of_data(object):
    &#34;&#34;&#34;
    Collection of data with each method as a stage of data manipulation process or metrics output.
    &#34;&#34;&#34;

    def __init__(self, DataInput):
        # first validating the input to see which method to use
        inputType = os.path.splitext(DataInput)[-1]
        if inputType == &#34;.txt&#34;:
            read_method = input_methods.get(&#39;txt&#39;)
        else:
            read_method = input_methods.get(&#39;&#39;)

        # Actually initiating the object attributes
        self._DataInput = DataInput
        self._RawData = read_method(DataInput)

    def cleaning(self):
        &#34;&#34;&#34;
        Performing all the cleanings related to this dataset
        &#34;&#34;&#34;
        df = self._RawData.drop_duplicates().copy() #Creating a copy of the drop_dup dataframe
        # Run a for-loop to go through string columns
        # and strip the leading and trailing whitespaces

        for i in df.columns:
            if is_string_dtype(df[i]):   # if it&#39;s a string column
                df[i] = df[i].str.strip()   # strip out the white spaces
            else:
                pass
    
        # resetting index after the drop duplicates
        df = df.reset_index(drop=True)

        # Cleaning up the duplicated states in cell
        df[&#39;city_state&#39;] = df[&#39;city_state&#39;].str.replace(&#34;WASHINGTON, DC, DC&#34;, &#34;WASHINGTON DC, DC&#34;)
        df[&#39;city_state&#39;] = df[&#39;city_state&#39;].str.replace(&#34;KNOXVILLE, TN, TN&#34;, &#34;KNOXVILLE, TN&#34;)

        # Performing the zip code cleaning
        df[&#39;city_state&#39;] = df[&#39;city_state&#39;].replace(cleaningDict)

        # Saving it to the object
        self._clean_df = df

    def other_data_validations(self):
        &#34;&#34;&#34;
        Ensure new incoming data does not have new violations or needed updates.
        &#34;&#34;&#34;
        checker = []
        # Checking if the broker ID is number
        checker.append(is_numeric_dtype(self._clean_df[&#39;broker_name&#39;].str.split(&#34;Broker&#34;, expand=True)[1].astype(float)))
        # Checking if the broker name is correct
        checker.append(len(self._clean_df[&#39;broker_name&#39;].str.split(&#34;Broker&#34;, expand=True).columns) == 2)
        # Checking if the city states only has one comma
        checker.append(len(self._clean_df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True).columns) == 2)
        # Checking if the city state column contains numbers (zip codes)
        mask = self._clean_df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True)[0].str.isnumeric()
        checker.append(len(self._clean_df[mask]) == 0)
        # Checking if thet prefix is a character per describe
        mask1 = self._clean_df[&#39;territory&#39;].str[0].str.isalpha() == False
        mask2 = self._clean_df[&#39;territory&#39;].str[0] != &#34;I&#34;
        mask3 = self._clean_df[&#39;territory&#39;].str[0] != &#34;W&#34;
        checker.append(len(self._clean_df.loc[mask1, &#39;territory&#39;]) == 0)
        checker.append(len(self._clean_df.loc[mask2&amp;mask3, &#39;territory&#39;]) == 0)
        # Checking fund category column
        mask4 = self._clean_df[&#39;fund_category&#39;].str.isnumeric() == True
        checker.append(len(self._clean_df.loc[mask, &#39;fund_category&#39;]) == 0)
        checker.append(is_numeric_dtype(self._clean_df[&#39;firm_x_sales&#39;]))
        checker.append(is_numeric_dtype(self._clean_df[&#39;total_industry_sales&#39;]))

        # Checking binary columns
        bin_check = True
        for i in driver_columns:
            if len(self._clean_df[i].unique()) == 2:
                pass
            else:
                bin_check = False
                print(f&#34;{i} is not binary&#34;)
                break

        checker.append(bin_check)

        return checker


    def enrichment(self):
        &#34;&#34;&#34;
        Built on top of the clean_df, this is going to split out the analytics
        columns
        &#34;&#34;&#34;
        # Creating a copy of the clean dataframe for enrichment
        self.cleaning()
        df = self._clean_df.copy()

        # Binary encode all the binary categorical variables
        df[driver_columns] = df[driver_columns].replace(binary_encoding_map)
        
        # Splitting out the sates since the city_state information is too granular for overview
        df[&#39;state&#39;] = df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True)[1]
        
        # Splitting out the Channel
        df[&#39;i_or_w&#39;] =df[&#39;territory&#39;].str[0]
        I_OR_W = {
            &#34;I&#34; : 1,
            &#34;W&#34; : 0
        }

        df[&#39;i_or_w&#39;] = df[&#39;i_or_w&#39;].replace(I_OR_W)
        
        # Initiating a filter to exclude all the no sales generated record
        mask = df[&#39;firm_x_sales&#39;] &gt; 0
        significant_cut = df[mask][&#39;firm_x_sales&#39;].quantile(0.2)
        # Intiating a filter with the twenty percentile cut
        mask1 = df[&#39;firm_x_sales&#39;] &gt; significant_cut
        # Performing the filter and label
        df[&#39;effective_sale&#39;] = 0
        df.loc[mask1,&#39;effective_sale&#39;] = 1
        
        # Since driver_columns are the same as driver columns
        # we will utilize that to create driver_pattern
        df[&#39;driver_pattern&#39;] = df[driver_columns].apply(tuple,axis=1)
        
        df = df.reset_index(drop=True)
        # Save the enriched dataframe to the house
        self._enriched_df = df
    



    def get_metrics(self):
        &#34;&#34;&#34;
        Printing an output of quick metrics
        &#34;&#34;&#34;
        pass</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="data_processing.house_of_data.cleaning"><code class="name flex">
<span>def <span class="ident">cleaning</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Performing all the cleanings related to this dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cleaning(self):
    &#34;&#34;&#34;
    Performing all the cleanings related to this dataset
    &#34;&#34;&#34;
    df = self._RawData.drop_duplicates().copy() #Creating a copy of the drop_dup dataframe
    # Run a for-loop to go through string columns
    # and strip the leading and trailing whitespaces

    for i in df.columns:
        if is_string_dtype(df[i]):   # if it&#39;s a string column
            df[i] = df[i].str.strip()   # strip out the white spaces
        else:
            pass

    # resetting index after the drop duplicates
    df = df.reset_index(drop=True)

    # Cleaning up the duplicated states in cell
    df[&#39;city_state&#39;] = df[&#39;city_state&#39;].str.replace(&#34;WASHINGTON, DC, DC&#34;, &#34;WASHINGTON DC, DC&#34;)
    df[&#39;city_state&#39;] = df[&#39;city_state&#39;].str.replace(&#34;KNOXVILLE, TN, TN&#34;, &#34;KNOXVILLE, TN&#34;)

    # Performing the zip code cleaning
    df[&#39;city_state&#39;] = df[&#39;city_state&#39;].replace(cleaningDict)

    # Saving it to the object
    self._clean_df = df</code></pre>
</details>
</dd>
<dt id="data_processing.house_of_data.enrichment"><code class="name flex">
<span>def <span class="ident">enrichment</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Built on top of the clean_df, this is going to split out the analytics
columns</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def enrichment(self):
    &#34;&#34;&#34;
    Built on top of the clean_df, this is going to split out the analytics
    columns
    &#34;&#34;&#34;
    # Creating a copy of the clean dataframe for enrichment
    self.cleaning()
    df = self._clean_df.copy()

    # Binary encode all the binary categorical variables
    df[driver_columns] = df[driver_columns].replace(binary_encoding_map)
    
    # Splitting out the sates since the city_state information is too granular for overview
    df[&#39;state&#39;] = df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True)[1]
    
    # Splitting out the Channel
    df[&#39;i_or_w&#39;] =df[&#39;territory&#39;].str[0]
    I_OR_W = {
        &#34;I&#34; : 1,
        &#34;W&#34; : 0
    }

    df[&#39;i_or_w&#39;] = df[&#39;i_or_w&#39;].replace(I_OR_W)
    
    # Initiating a filter to exclude all the no sales generated record
    mask = df[&#39;firm_x_sales&#39;] &gt; 0
    significant_cut = df[mask][&#39;firm_x_sales&#39;].quantile(0.2)
    # Intiating a filter with the twenty percentile cut
    mask1 = df[&#39;firm_x_sales&#39;] &gt; significant_cut
    # Performing the filter and label
    df[&#39;effective_sale&#39;] = 0
    df.loc[mask1,&#39;effective_sale&#39;] = 1
    
    # Since driver_columns are the same as driver columns
    # we will utilize that to create driver_pattern
    df[&#39;driver_pattern&#39;] = df[driver_columns].apply(tuple,axis=1)
    
    df = df.reset_index(drop=True)
    # Save the enriched dataframe to the house
    self._enriched_df = df</code></pre>
</details>
</dd>
<dt id="data_processing.house_of_data.get_metrics"><code class="name flex">
<span>def <span class="ident">get_metrics</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Printing an output of quick metrics</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metrics(self):
    &#34;&#34;&#34;
    Printing an output of quick metrics
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="data_processing.house_of_data.other_data_validations"><code class="name flex">
<span>def <span class="ident">other_data_validations</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Ensure new incoming data does not have new violations or needed updates.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def other_data_validations(self):
    &#34;&#34;&#34;
    Ensure new incoming data does not have new violations or needed updates.
    &#34;&#34;&#34;
    checker = []
    # Checking if the broker ID is number
    checker.append(is_numeric_dtype(self._clean_df[&#39;broker_name&#39;].str.split(&#34;Broker&#34;, expand=True)[1].astype(float)))
    # Checking if the broker name is correct
    checker.append(len(self._clean_df[&#39;broker_name&#39;].str.split(&#34;Broker&#34;, expand=True).columns) == 2)
    # Checking if the city states only has one comma
    checker.append(len(self._clean_df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True).columns) == 2)
    # Checking if the city state column contains numbers (zip codes)
    mask = self._clean_df[&#39;city_state&#39;].str.split(&#34;, &#34;, expand=True)[0].str.isnumeric()
    checker.append(len(self._clean_df[mask]) == 0)
    # Checking if thet prefix is a character per describe
    mask1 = self._clean_df[&#39;territory&#39;].str[0].str.isalpha() == False
    mask2 = self._clean_df[&#39;territory&#39;].str[0] != &#34;I&#34;
    mask3 = self._clean_df[&#39;territory&#39;].str[0] != &#34;W&#34;
    checker.append(len(self._clean_df.loc[mask1, &#39;territory&#39;]) == 0)
    checker.append(len(self._clean_df.loc[mask2&amp;mask3, &#39;territory&#39;]) == 0)
    # Checking fund category column
    mask4 = self._clean_df[&#39;fund_category&#39;].str.isnumeric() == True
    checker.append(len(self._clean_df.loc[mask, &#39;fund_category&#39;]) == 0)
    checker.append(is_numeric_dtype(self._clean_df[&#39;firm_x_sales&#39;]))
    checker.append(is_numeric_dtype(self._clean_df[&#39;total_industry_sales&#39;]))

    # Checking binary columns
    bin_check = True
    for i in driver_columns:
        if len(self._clean_df[i].unique()) == 2:
            pass
        else:
            bin_check = False
            print(f&#34;{i} is not binary&#34;)
            break

    checker.append(bin_check)

    return checker</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="data_processing.binary_encoding_map" href="#data_processing.binary_encoding_map">binary_encoding_map</a></code></li>
<li><code><a title="data_processing.cleaningDict" href="#data_processing.cleaningDict">cleaningDict</a></code></li>
<li><code><a title="data_processing.driver_columns" href="#data_processing.driver_columns">driver_columns</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="data_processing.read_all_files" href="#data_processing.read_all_files">read_all_files</a></code></li>
<li><code><a title="data_processing.read_file" href="#data_processing.read_file">read_file</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="data_processing.house_of_data" href="#data_processing.house_of_data">house_of_data</a></code></h4>
<ul class="">
<li><code><a title="data_processing.house_of_data.cleaning" href="#data_processing.house_of_data.cleaning">cleaning</a></code></li>
<li><code><a title="data_processing.house_of_data.enrichment" href="#data_processing.house_of_data.enrichment">enrichment</a></code></li>
<li><code><a title="data_processing.house_of_data.get_metrics" href="#data_processing.house_of_data.get_metrics">get_metrics</a></code></li>
<li><code><a title="data_processing.house_of_data.other_data_validations" href="#data_processing.house_of_data.other_data_validations">other_data_validations</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>